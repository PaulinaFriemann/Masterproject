\subsection{Markov Decision Problems}

The problem at hand is defined by \citeNP{baker2009} as a Markov Decision Problem (MDP; \citeNP{bellman1954}, \citeNP[p.~500 ff.]{russell1995}), even though the model itself has deterministic transitions instead of stochastic transitions which MDPs are usually used for. This has several advantages over classical path-finding algorithms in this scenario: First, it is possible to directly infer decision probabilities from the utility function, whereas classical path-finding algorithms usually provide action sequences, and probability functions would have to be calculated, leaving a large decision to the modeler. Second, it is possible to calculate a path to the goal from every state, therefore solving the Single-Destination Shortest Path problem. Third, it makes it possible to look at arbitrary partial trajectories without defining the end state of a particular trajectory as a goal, thereby allowing goal inferences for partial trajectories. Fourth, it is easy to change the model in the future, so for example to include stochastic transitions.

The main difference between classic path-finding problems and MDPs is that policies are calculated instead of action sequences, because action outcomes are stochastic.

In the following, the basics of MDPs will be defined after \citeNP[p.~500ff.]{russell1995}. 

\paragraph{Transition Model.}

A transition model describes the transition probabilities between states. $M^a_{ij}$ is the probability of reaching state $j$ from state $i$ by taking action $a$. Generally, \textit{Markov Property} holds, which means that transition probabilities are only dependent on the current state, and not on previous states in the history.

\paragraph{Policy.}
A policy $\pi$ is a mapping from a set of states $S$ to a set of actions $A$ \cite{russell1995} (p.500). Finding the optimal path translates to finding the optimal policy $\pi^*$, so choosing the optimal action for every state. This is done by assigning utilities to all states by determining the expected utility of being in a state $i$ and executing an optimal policy $\pi^*$.
This translates to the expected utility over all possible environment histories $H(i, \pi^*)$ generated by that policy after starting in state $i$:

\begin{equation}
	U(i) = EU(H(i, \pi^*)|M) = \sum P(H(i, \pi^*)|M)U_h(H(i,\pi^*))
\end{equation}

Here, $U_h$ denotes the utility functions on histories, and needs to be separable, meaning there exists a function $f$ such that $U_h([s_0, s_1,...,s_n]) = f(s_0, U_h(s_1, ..., s_n))$.
Taking rational then translates to choosing actions which promise the highest utility, according to the Maximum Expected Utility principle.

\paragraph{Markov Decision Problems.}

Markov Decision Problems are the problem of finding an optimal policy in a fully observable environment, whereby transitions in this environment are stochastic, but the transition model is known.


\paragraph{Value Iteration Algorithm.}\label{para:value_it}

To calculate an optimal policy, a \textit{dynamic programming} algorithm can be employed. One such algorithm is the \textit{Value Iteration algorithm}.

Let $R: S \rightarrow {\rm I\!R}$ be a \textit{reward function} that maps all states in the environment to a real value, for example based on move cost to this state, and whether the state is a goal or an obstacle.

Using the last definitions, and a utility function on histories which is additive, meaning $U_h([s_0, s_1,...,s_n]) = R(s_0) + U_h(s_1, ..., s_n)$. We can then equate the optimal policy $\pi^*$ of a state $i$ to 

\begin{equation}\label{eq:optimalpolicy}
	\pi^*(i) = \argmax_a \sum_{j} M_{ij}^aU(j)
\end{equation}
	

The utility of a state $U(i)$ can similarly be defined as 

\begin{equation}
U(i)  = R(i) + \max_a \sum_j   M_{ij}^aU(j)
\end{equation}

When applying this equation to all states repeatedly, the utilities eventually converge. From these utilities, the optimal policy can be chosen according to Equation \ref{eq:optimalpolicy}.